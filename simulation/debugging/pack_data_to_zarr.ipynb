{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing Data Into Provided Sample Zarr\n",
    "\n",
    "The point of this notebook is to prototype the packaging of the simulation data I generated via Blender into a dataset Zarr tree like what Cheng provided on his GitHub. This way, we can easily integrate the simulation into the existing data structure for training the tracking extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import zarr\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import pandas as pd\n",
    "from numcodecs import Blosc\n",
    "\n",
    "# Do some ugly path manipulation to find all packages\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "from simulation.io_util.image_io_util import read_uviz\n",
    "from simulation.common.projection import ray_length_to_zbuffer, zbuffer_to_pcloud\n",
    "from simulation.common.geometry_util import (barycentric_interpolation, get_aabb, get_union_aabb)\n",
    "from simulation.cloth_3d_util.util import quads2tris, axis_angle_to_matrix\n",
    "from simulation.common.igl_util import query_uv_barycentric\n",
    "from simulation.data_packager.cloth_3d_canonical_accessor import Cloth3DCanonicalAccessor\n",
    "\n",
    "# Trying to avoid this import as it will execute the Zarr packaging code as is.\n",
    "# from simulation.data_packager.smpl_cloth_zarr_v5_cheng import Cloth3DCanonicalAccessor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File root: /home/dcolli23/code/school/rob599_deeprob/projects/final/garmentnets_tracking/simulation/data_packager\n",
      "GarmentNets root: /home/dcolli23/code/school/rob599_deeprob/projects/final/garmentnets_tracking\n"
     ]
    }
   ],
   "source": [
    "FILE_ROOT = Path(os.getcwd())\n",
    "print(\"File root:\", FILE_ROOT)\n",
    "GARMENTNETS_ROOT = (FILE_ROOT / \"..\" / \"..\").absolute().resolve()\n",
    "print(\"GarmentNets root:\", GARMENTNETS_ROOT)\n",
    "SIM_DATASET_DIR = (FILE_ROOT / \"..\" / \"script_output\" / \"full_dataset_attempt_2\").absolute()\n",
    "CLOTH3D_PATH = Path(os.path.expanduser(\"~/DataLocker/datasets/CLOTH3D/training/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for debugging\n",
    "def print_nested_dict_types(d: dict):\n",
    "    keys_and_values = [(k, v, 0) for k, v in d.items()][::-1]\n",
    "    while len(keys_and_values) > 0:\n",
    "        key, val, level = keys_and_values.pop(-1)\n",
    "        # if\n",
    "        tabs = '\\t' * level\n",
    "        if isinstance(val, dict):\n",
    "            print(tabs + key)\n",
    "            for subkey, subval in val.items():\n",
    "                keys_and_values.append((subkey, subval, level + 1))\n",
    "        else:\n",
    "            print(tabs + key, type(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs:\n",
    "\n",
    "- I should read the Blend files so that I can calculate exactly what the change in position/velocity of the \"gripper\" is at each timestep.\n",
    "- Define interface for new Zarr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Zarr Interface\n",
    "\n",
    "refer to `zarr_structure_plan.md` for new planned structure.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Zarr\n",
    "\n",
    "- Use `<zarr_node>.create_group('group_name')` to create a group at the current node (group)\n",
    "- Use `<var> = <group>.array(name='name', data=<your_numpy_array>)` to add an array to a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZARR_FILEPATH = GARMENTNETS_ROOT / \"data\" / \"garmentnets_tracking_simulation_dataset.zarr\"\n",
    "\n",
    "# r+ is read/write but the data must exist first.\n",
    "# mode = 'r'  # Use for debugging first.\n",
    "# mode = 'r+'\n",
    "# zr = zarr.open(ZARR_FILEPATH.as_posix(), mode=mode)\n",
    "compressor = Blosc(cname='zstd', clevel=6, shuffle=Blosc.BITSHUFFLE)\n",
    "\n",
    "categories = [\"Tshirt\"]\n",
    "rows = dict()\n",
    "for category in categories:\n",
    "    store = zarr.DirectoryStore(ZARR_FILEPATH / category)\n",
    "    root = zarr.group(store=store, overwrite=False)\n",
    "    sample_root = root.require_group('samples', overwrite=False)\n",
    "    summary_root = root.require_group('summary', overwrite=False)\n",
    "    rows[category] = {\n",
    "        'compressor': compressor,\n",
    "        'store': store,\n",
    "        'root': root,\n",
    "        'sample_root': sample_root,\n",
    "        'summary_root': summary_root\n",
    "    }\n",
    "datasets_df = pd.DataFrame(list(rows.values()), index=rows.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compressor</th>\n",
       "      <th>store</th>\n",
       "      <th>root</th>\n",
       "      <th>sample_root</th>\n",
       "      <th>summary_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tshirt</th>\n",
       "      <td>Blosc(cname='zstd', clevel=6, shuffle=BITSHUFF...</td>\n",
       "      <td>[.zgroup, summary/.zgroup, samples/.zgroup]</td>\n",
       "      <td>[samples, summary]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               compressor  \\\n",
       "Tshirt  Blosc(cname='zstd', clevel=6, shuffle=BITSHUFF...   \n",
       "\n",
       "                                              store                root  \\\n",
       "Tshirt  [.zgroup, summary/.zgroup, samples/.zgroup]  [samples, summary]   \n",
       "\n",
       "       sample_root summary_root  \n",
       "Tshirt          []           []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zarr.hierarchy.Group"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping Dynamics Zarr Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_seq_dir = SIM_DATASET_DIR / \"00380_Tshirt_509\" / \"dynamics_seq_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the simulation results for this dynamics run\n",
    "results_filepath = dynamics_seq_dir / \"dynamics_sim_results.pkl\"\n",
    "results_dict = pickle.load(results_filepath.open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "meta_path = dynamics_seq_dir.joinpath('meta.pk')\n",
    "meta = pickle.load(meta_path.open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoading rgb and uviz for frame 0\n",
      "\tLoading rgb and uviz for frame 1\n",
      "\tLoading rgb and uviz for frame 2\n",
      "\tLoading rgb and uviz for frame 3\n",
      "\tLoading rgb and uviz for frame 4\n",
      "\tLoading rgb and uviz for frame 5\n",
      "\tLoading rgb and uviz for frame 6\n",
      "\tLoading rgb and uviz for frame 7\n",
      "\tLoading rgb and uviz for frame 8\n",
      "\tLoading rgb and uviz for frame 9\n",
      "\tLoading rgb and uviz for frame 10\n",
      "\tLoading rgb and uviz for frame 11\n",
      "\tLoading rgb and uviz for frame 12\n",
      "\tLoading rgb and uviz for frame 13\n",
      "\tLoading rgb and uviz for frame 14\n",
      "\tLoading rgb and uviz for frame 15\n",
      "\tLoading rgb and uviz for frame 16\n",
      "\tLoading rgb and uviz for frame 17\n",
      "\tLoading rgb and uviz for frame 18\n",
      "\tLoading rgb and uviz for frame 19\n",
      "\tLoading rgb and uviz for frame 20\n",
      "\tLoading rgb and uviz for frame 21\n",
      "\tLoading rgb and uviz for frame 22\n",
      "\tLoading rgb and uviz for frame 23\n",
      "\tLoading rgb and uviz for frame 24\n",
      "\tLoading rgb and uviz for frame 25\n",
      "\tLoading rgb and uviz for frame 26\n",
      "\tLoading rgb and uviz for frame 27\n",
      "\tLoading rgb and uviz for frame 28\n",
      "\tLoading rgb and uviz for frame 29\n",
      "\tLoading rgb and uviz for frame 30\n",
      "\tLoading rgb and uviz for frame 31\n",
      "\tLoading rgb and uviz for frame 32\n",
      "\tLoading rgb and uviz for frame 33\n",
      "\tLoading rgb and uviz for frame 34\n",
      "\tLoading rgb and uviz for frame 35\n",
      "\tLoading rgb and uviz for frame 36\n",
      "\tLoading rgb and uviz for frame 37\n",
      "\tLoading rgb and uviz for frame 38\n",
      "\tLoading rgb and uviz for frame 39\n",
      "\tLoading rgb and uviz for frame 40\n",
      "\tLoading rgb and uviz for frame 41\n",
      "\tLoading rgb and uviz for frame 42\n",
      "\tLoading rgb and uviz for frame 43\n",
      "\tLoading rgb and uviz for frame 44\n",
      "\tLoading rgb and uviz for frame 45\n",
      "\tLoading rgb and uviz for frame 46\n",
      "\tLoading rgb and uviz for frame 47\n",
      "\tLoading rgb and uviz for frame 48\n",
      "\tLoading rgb and uviz for frame 49\n",
      "\tLoading rgb and uviz for frame 50\n",
      "\tLoading rgb and uviz for frame 51\n",
      "\tLoading rgb and uviz for frame 52\n",
      "\tLoading rgb and uviz for frame 53\n",
      "\tLoading rgb and uviz for frame 54\n",
      "\tLoading rgb and uviz for frame 55\n",
      "\tLoading rgb and uviz for frame 56\n",
      "\tLoading rgb and uviz for frame 57\n",
      "\tLoading rgb and uviz for frame 58\n",
      "\tLoading rgb and uviz for frame 59\n",
      "\tLoading rgb and uviz for frame 60\n",
      "\tLoading rgb and uviz for frame 61\n",
      "\tLoading rgb and uviz for frame 62\n",
      "\tLoading rgb and uviz for frame 63\n",
      "\tLoading rgb and uviz for frame 64\n",
      "\tLoading rgb and uviz for frame 65\n",
      "\tLoading rgb and uviz for frame 66\n",
      "\tLoading rgb and uviz for frame 67\n",
      "\tLoading rgb and uviz for frame 68\n",
      "\tLoading rgb and uviz for frame 69\n",
      "\tLoading rgb and uviz for frame 70\n",
      "\tLoading rgb and uviz for frame 71\n",
      "\tLoading rgb and uviz for frame 72\n",
      "\tLoading rgb and uviz for frame 73\n",
      "\tLoading rgb and uviz for frame 74\n",
      "\tLoading rgb and uviz for frame 75\n",
      "\tLoading rgb and uviz for frame 76\n",
      "\tLoading rgb and uviz for frame 77\n",
      "\tLoading rgb and uviz for frame 78\n",
      "\tLoading rgb and uviz for frame 79\n",
      "\tLoading rgb and uviz for frame 80\n",
      "\tLoading rgb and uviz for frame 81\n",
      "\tLoading rgb and uviz for frame 82\n",
      "\tLoading rgb and uviz for frame 83\n",
      "\tLoading rgb and uviz for frame 84\n",
      "\tLoading rgb and uviz for frame 85\n",
      "\tLoading rgb and uviz for frame 86\n",
      "\tLoading rgb and uviz for frame 87\n",
      "\tLoading rgb and uviz for frame 88\n",
      "\tLoading rgb and uviz for frame 89\n",
      "\tLoading rgb and uviz for frame 90\n",
      "\tLoading rgb and uviz for frame 91\n",
      "\tLoading rgb and uviz for frame 92\n",
      "\tLoading rgb and uviz for frame 93\n",
      "\tLoading rgb and uviz for frame 94\n",
      "\tLoading rgb and uviz for frame 95\n",
      "\tLoading rgb and uviz for frame 96\n",
      "\tLoading rgb and uviz for frame 97\n",
      "\tLoading rgb and uviz for frame 98\n",
      "\tLoading rgb and uviz for frame 99\n",
      "\tLoading rgb and uviz for frame 100\n",
      "\tLoading rgb and uviz for frame 101\n",
      "\tLoading rgb and uviz for frame 102\n",
      "\tLoading rgb and uviz for frame 103\n",
      "\tLoading rgb and uviz for frame 104\n",
      "\tLoading rgb and uviz for frame 105\n",
      "\tLoading rgb and uviz for frame 106\n",
      "\tLoading rgb and uviz for frame 107\n",
      "\tLoading rgb and uviz for frame 108\n",
      "\tLoading rgb and uviz for frame 109\n",
      "\tLoading rgb and uviz for frame 110\n",
      "\tLoading rgb and uviz for frame 111\n",
      "\tLoading rgb and uviz for frame 112\n",
      "\tLoading rgb and uviz for frame 113\n",
      "\tLoading rgb and uviz for frame 114\n",
      "\tLoading rgb and uviz for frame 115\n",
      "\tLoading rgb and uviz for frame 116\n",
      "\tLoading rgb and uviz for frame 117\n",
      "\tLoading rgb and uviz for frame 118\n",
      "\tLoading rgb and uviz for frame 119\n",
      "\tLoading rgb and uviz for frame 120\n",
      "\tLoading rgb and uviz for frame 121\n",
      "\tLoading rgb and uviz for frame 122\n",
      "\tLoading rgb and uviz for frame 123\n",
      "\tLoading rgb and uviz for frame 124\n",
      "\tLoading rgb and uviz for frame 125\n",
      "\tLoading rgb and uviz for frame 126\n",
      "\tLoading rgb and uviz for frame 127\n",
      "\tLoading rgb and uviz for frame 128\n",
      "\tLoading rgb and uviz for frame 129\n",
      "\tLoading rgb and uviz for frame 130\n",
      "\tLoading rgb and uviz for frame 131\n",
      "\tLoading rgb and uviz for frame 132\n",
      "\tLoading rgb and uviz for frame 133\n",
      "\tLoading rgb and uviz for frame 134\n",
      "\tLoading rgb and uviz for frame 135\n",
      "\tLoading rgb and uviz for frame 136\n",
      "\tLoading rgb and uviz for frame 137\n",
      "\tLoading rgb and uviz for frame 138\n",
      "\tLoading rgb and uviz for frame 139\n",
      "\tLoading rgb and uviz for frame 140\n",
      "\tLoading rgb and uviz for frame 141\n",
      "\tLoading rgb and uviz for frame 142\n",
      "\tLoading rgb and uviz for frame 143\n",
      "\tLoading rgb and uviz for frame 144\n",
      "\tLoading rgb and uviz for frame 145\n",
      "\tLoading rgb and uviz for frame 146\n",
      "\tLoading rgb and uviz for frame 147\n",
      "\tLoading rgb and uviz for frame 148\n",
      "\tLoading rgb and uviz for frame 149\n",
      "\tLoading rgb and uviz for frame 150\n",
      "\tLoading rgb and uviz for frame 151\n",
      "\tLoading rgb and uviz for frame 152\n"
     ]
    }
   ],
   "source": [
    "# Tentative: load the canonical data? Or maybe pass it in if we need it to get canonical\n",
    "# coordinates corresponding to the points in the point cloud of the dynamics sequence.\n",
    "\n",
    "# Load in the images.\n",
    "uviz_fpaths = list(dynamics_seq_dir.glob(\"*.exr\"))\n",
    "rgb_fpaths = list(dynamics_seq_dir.glob(\"*.png\"))\n",
    "assert (len(uviz_fpaths) == len(rgb_fpaths))\n",
    "rows = list()\n",
    "for i, (uviz_fpath, rgb_fpath) in enumerate(zip(uviz_fpaths, rgb_fpaths)):\n",
    "    print(f\"\\tLoading rgb and uviz for frame {i}\")\n",
    "    uviz_dict = read_uviz(str(uviz_fpath.absolute()), index_dtype=np.uint8)\n",
    "    rgb = skimage.io.imread(str(rgb_fpath.absolute()))\n",
    "\n",
    "    row = uviz_dict\n",
    "    row['rgb'] = rgb\n",
    "    rows.append(row)\n",
    "\n",
    "    # Only load up to the images that we have animations for.\n",
    "    if i == results_dict[\"simulation_info\"][\"frame_end\"]:\n",
    "        break\n",
    "\n",
    "images_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the images.\n",
    "intrinsic = meta['camera']['intrinsic']\n",
    "extrinsic_arr = np.array(meta['camera']['extrinsic_list'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_arr = np.array(list(images_df.rgb))\n",
    "uv_arr = np.array(list(images_df.uv))\n",
    "index_arr = np.array(list(images_df.object_index))\n",
    "# should specify index in meta\n",
    "mask_arr = (index_arr == 1).squeeze()\n",
    "# convert Cycles ray length to CV depth\n",
    "# depth_arr = np.array(list(images_df.depth.apply(\n",
    "#     lambda x: ray_length_to_zbuffer(x, intrinsic))))\n",
    "# in Blender 2.90, Cycle's definition of depth changed to CV depth\n",
    "depth_arr = np.array(list(images_df.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb shape (153, 1024, 1024, 3)\n",
      "uv_arr shape (153, 1024, 1024, 2)\n",
      "index_arr shape (153, 1024, 1024, 1)\n",
      "mask_arr shape (153, 1024, 1024)\n",
      "depth_arr shape (153, 1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"rgb shape\", rgb_arr.shape)\n",
    "print(\"uv_arr shape\", uv_arr.shape)\n",
    "print(\"index_arr shape\", index_arr.shape)\n",
    "print(\"mask_arr shape\", mask_arr.shape)\n",
    "print(\"depth_arr shape\", depth_arr.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate point cloud in the global frame.\n",
    "# NOTE: Have to translate by the negative of the amount the camera was translated for dynamics\n",
    "# sequence recording.\n",
    "point_cloud_arr = np.empty(rgb_arr.shape, dtype=np.float16)\n",
    "assert(depth_arr.shape[0] == rgb_arr.shape[0])\n",
    "for i in range(len(depth_arr)):\n",
    "    depth = depth_arr[i]\n",
    "    extrinsic = extrinsic_arr[i]\n",
    "    pc_local = zbuffer_to_pcloud(depth, intrinsic)\n",
    "    tx_world_camera = np.linalg.inv(extrinsic)\n",
    "    pc_global = pc_local @ tx_world_camera[:3,:3].T + tx_world_camera[:3, 3]\n",
    "    point_cloud_arr[i] = pc_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Following Should Be Converted To A Function To Add A Single Sample's Simulations To Zarr\n",
    "\n",
    "This was taken from `smpl_cloth_zarr_v5_cheng.convert_experiment()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some debug variables that will eventually be used as function arguments.\n",
    "sample_dir = SIM_DATASET_DIR / \"00380_Tshirt_509\"\n",
    "\n",
    "# zarr configuration\n",
    "sample_root = sample_root  # Probably the \"Group\" at which this sample is located.\n",
    "compressor = compressor\n",
    "accessor = Cloth3DCanonicalAccessor(CLOTH3D_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "meta_path = sample_dir.joinpath('meta.pk')\n",
    "sim_result_path = sample_dir.joinpath('hanging_rest_state_results.pkl')\n",
    "meta = pickle.load(meta_path.open('rb'))\n",
    "sim_result = pickle.load(sim_result_path.open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera\n",
      "\textrinsic_list <class 'list'>\n",
      "\tintrinsic <class 'numpy.ndarray'>\n",
      "images\n",
      "\trgb <class 'list'>\n",
      "\tuviz <class 'list'>\n",
      "meta\n",
      "\tfabric <class 'str'>\n",
      "\tgender <class 'int'>\n",
      "\tgarment_name <class 'str'>\n",
      "\tsample_id <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print_nested_dict_types(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and read metadata\n",
    "cloth_verts = sim_result['cloth_state']['verts']\n",
    "cloth_faces = np.array(sim_result['cloth_state']['faces'])\n",
    "# CLOTH3D data has all quad faces\n",
    "assert(cloth_faces.shape[1] == 4)\n",
    "cloth_uv_verts = sim_result['cloth_state']['uv_verts']\n",
    "cloth_uv_faces = np.array(sim_result['cloth_state']['uv_faces'])\n",
    "assert(cloth_uv_faces.shape[1] == 4)\n",
    "grip_vertex_idx = sim_result['grip_vertex_idx']\n",
    "assert(0 <= grip_vertex_idx < len(cloth_verts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load canonical data\n",
    "canonical_data = accessor.get_sample_data(\n",
    "    sample_id=meta['meta']['sample_id'],\n",
    "    garment_name=meta['meta']['garment_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute per-cloth-vertex nearest human vertex\n",
    "human_verts = canonical_data['human_verts']\n",
    "human_faces = canonical_data['human_faces']\n",
    "cloth_canonical_verts = canonical_data['garment_verts']\n",
    "cloth_texture = canonical_data['garment_texture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "# TODO: This will have to be heavily modified to work with the dynamics simulations.\n",
    "uviz_fnames = meta['images']['uviz']\n",
    "rgb_fnames = meta['images']['rgb']\n",
    "assert(len(uviz_fnames) == len(rgb_fnames))\n",
    "rows = list()\n",
    "for uviz_fname, rgb_fname in zip(uviz_fnames, rgb_fnames):\n",
    "    uviz_path = sample_dir.joinpath(uviz_fname)\n",
    "    rgb_path = sample_dir.joinpath(rgb_fname)\n",
    "    uviz_dict = read_uviz(str(uviz_path.absolute()), index_dtype=np.uint8)\n",
    "    rgb = skimage.io.imread(str(rgb_path.absolute()))\n",
    "\n",
    "    row = uviz_dict\n",
    "    row['rgb'] = rgb\n",
    "    rows.append(row)\n",
    "images_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat images\n",
    "intrinsic = meta['camera']['intrinsic']\n",
    "extrinsic_arr = np.array(list(meta['camera']['extrinsic_list']))\n",
    "rgb_arr = np.array(list(images_df.rgb))\n",
    "uv_arr = np.array(list(images_df.uv))\n",
    "index_arr = np.array(list(images_df.object_index))\n",
    "# should specify index in meta\n",
    "mask_arr = (index_arr == 1).squeeze()\n",
    "# convert Cycles ray length to CV depth\n",
    "# depth_arr = np.array(list(images_df.depth.apply(\n",
    "#     lambda x: ray_length_to_zbuffer(x, intrinsic))))\n",
    "# in Blender 2.90, Cycle's definition of depth changed to CV depth\n",
    "depth_arr = np.array(list(images_df.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate point cloud in global frame\n",
    "point_cloud_arr = np.empty(rgb_arr.shape, dtype=np.float16)\n",
    "assert(len(depth_arr) == len(extrinsic_arr))\n",
    "for i in range(len(depth_arr)):\n",
    "    depth = depth_arr[i]\n",
    "    extrinsic = extrinsic_arr[i]\n",
    "    pc_local = zbuffer_to_pcloud(depth, intrinsic)\n",
    "    tx_world_camera = np.linalg.inv(extrinsic)\n",
    "    pc_global = pc_local @ tx_world_camera[:3,:3].T + tx_world_camera[:3, 3]\n",
    "    point_cloud_arr[i] = pc_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cloth point cloud\n",
    "pc_points = point_cloud_arr[mask_arr]\n",
    "pc_uv = uv_arr[mask_arr]\n",
    "pc_rgb = rgb_arr[mask_arr]\n",
    "pc_sizes = np.sum(mask_arr, (1,2))\n",
    "assert(np.sum(pc_sizes) == len(pc_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute canonical coordinate for point cloud\n",
    "cloth_uv_faces_tri = quads2tris(cloth_uv_faces)\n",
    "cloth_faces_tri = quads2tris(cloth_faces)\n",
    "\n",
    "query_uv = pc_uv\n",
    "target_uv_verts = cloth_uv_verts\n",
    "target_uv_faces = cloth_uv_faces_tri\n",
    "\n",
    "barycentric, proj_face_idx = query_uv_barycentric(pc_uv, cloth_uv_verts, cloth_uv_faces_tri)\n",
    "pc_canonical = barycentric_interpolation(\n",
    "    barycentric, cloth_canonical_verts, cloth_faces_tri[proj_face_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute human/cloth aabb\n",
    "human_aabb = get_aabb(human_verts)\n",
    "cloth_aabb = get_aabb(cloth_canonical_verts)\n",
    "aabb = get_union_aabb(human_aabb, cloth_aabb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to zarr\n",
    "experiment_group = sample_root.require_group(sample_dir.stem, overwrite=False)\n",
    "image_group = experiment_group.require_group('image', overwrite=True)\n",
    "point_cloud_group = experiment_group.require_group('point_cloud', overwrite=True)\n",
    "misc_group = experiment_group.require_group('misc', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write misc arrays\n",
    "misc_data = {\n",
    "    'cloth_verts': cloth_verts.astype(np.float32),\n",
    "    'cloth_faces': cloth_faces.astype(np.uint16),\n",
    "    'cloth_uv_verts': cloth_uv_verts.astype(np.float32),\n",
    "    'cloth_uv_faces': cloth_uv_faces.astype(np.uint16),\n",
    "    'cloth_canonical_verts': cloth_canonical_verts.astype(np.float32),\n",
    "    'human_verts': human_verts.astype(np.float32),\n",
    "    'human_faces': human_faces.astype(np.uint16),\n",
    "    'cloth_aabb': cloth_aabb.astype(np.float32),\n",
    "    'human_aabb': human_aabb.astype(np.float32),\n",
    "    'intrinsic': intrinsic.astype(np.float32),\n",
    "    'extrinsic_list': extrinsic_arr.astype(np.float32),\n",
    "    'cloth_texture': cloth_texture.astype(np.uint8)\n",
    "}\n",
    "for key, data in misc_data.items():\n",
    "    misc_group.array(\n",
    "        name=key, data=data, chunks=data.shape,\n",
    "        compressor=compressor, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write image arrays\n",
    "image_data = {\n",
    "    'rgb': rgb_arr.astype(np.uint8),\n",
    "    'uv': uv_arr.astype(np.float16),\n",
    "    'depth': depth_arr.astype(np.float16),\n",
    "    'mask': np.expand_dims(mask_arr, axis=-1)\n",
    "}\n",
    "for key, data in image_data.items():\n",
    "    image_group.array(\n",
    "        name=key, data=data, chunks=(1,) + data.shape[1:],\n",
    "        compressor=compressor, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write point cloud arrays\n",
    "pc_data = {\n",
    "    'point': pc_points.astype(np.float16),\n",
    "    'uv': pc_uv.astype(np.float16),\n",
    "    'rgb': pc_rgb.astype(np.uint8),\n",
    "    'canonical_point': pc_canonical.astype(np.float16),\n",
    "    'sizes': pc_sizes.astype(np.int64)\n",
    "}\n",
    "for key, data in pc_data.items():\n",
    "    point_cloud_group.array(\n",
    "        name=key, data=data, chunks=data.shape,\n",
    "        compressor=compressor, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set attrs\n",
    "meta_attr = meta['meta']\n",
    "attrs = {\n",
    "    'sample_id': meta_attr['sample_id'],\n",
    "    'garment_name': meta_attr['garment_name'],\n",
    "    'gender': meta_attr['gender'],\n",
    "    'fabric': meta_attr['fabric'],\n",
    "    'grip_vertex_idx': grip_vertex_idx\n",
    "}\n",
    "experiment_group.attrs.put(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some garbage so that when I hit \"run all\" it errors on this cell and halts execution.\n",
    "asdfasdfasdfasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Simulation Images\n",
    "\n",
    "Image arrays:\n",
    "- `depth`\n",
    "- `mask`\n",
    "- `rgb`\n",
    "- `uv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Simulation Point Clouds\n",
    "\n",
    "These point clouds will be generated by using the camera intrinsic, the renders, and the masks from the renders. \n",
    "\n",
    "When saving the point clouds to the Zarr, the point clouds should be translated by the Z offset used to offset the camera. This was done so as to make sure the Tshirt fit in the camera frame across the full dynamics simulation (hopefully).\n",
    "\n",
    "Need the following from the simulation data:\n",
    "- `canonical_point`\n",
    "- `point`\n",
    "- `rgb`\n",
    "- `sizes`\n",
    "- `uv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"zarr-info\"><tbody><tr><th style=\"text-align: left\">Name</th><td style=\"text-align: left\">/Tshirt/samples/00380_Tshirt_509/image</td></tr><tr><th style=\"text-align: left\">Type</th><td style=\"text-align: left\">zarr.hierarchy.Group</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Store type</th><td style=\"text-align: left\">zarr.storage.DirectoryStore</td></tr><tr><th style=\"text-align: left\">No. members</th><td style=\"text-align: left\">4</td></tr><tr><th style=\"text-align: left\">No. arrays</th><td style=\"text-align: left\">4</td></tr><tr><th style=\"text-align: left\">No. groups</th><td style=\"text-align: left\">0</td></tr><tr><th style=\"text-align: left\">Arrays</th><td style=\"text-align: left\">depth, mask, rgb, uv</td></tr></tbody></table>"
      ],
      "text/plain": [
       "Name        : /Tshirt/samples/00380_Tshirt_509/image\n",
       "Type        : zarr.hierarchy.Group\n",
       "Read-only   : True\n",
       "Store type  : zarr.storage.DirectoryStore\n",
       "No. members : 4\n",
       "No. arrays  : 4\n",
       "No. groups  : 0\n",
       "Arrays      : depth, mask, rgb, uv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zr['Tshirt/samples/00380_Tshirt_509/image'].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative: Add Miscellaneous Info?\n",
    "\n",
    "Misc. Arrays:\n",
    "- `cloth_aabb`\n",
    "- `cloth_canonical_aabb`\n",
    "- `cloth_canonical_verts`\n",
    "- `cloth_faces`\n",
    "- `cloth_texture`\n",
    "- `cloth_uv_faces`\n",
    "- `cloth_uv_verts`\n",
    "- `cloth_verts`\n",
    "- `extrinsic_list`\n",
    "- `human_canonical_aabb`\n",
    "- `human_faces`\n",
    "- `human_verts`\n",
    "- `intrinsic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"zarr-info\"><tbody><tr><th style=\"text-align: left\">Name</th><td style=\"text-align: left\">/Tshirt/samples/00380_Tshirt_509/misc</td></tr><tr><th style=\"text-align: left\">Type</th><td style=\"text-align: left\">zarr.hierarchy.Group</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Store type</th><td style=\"text-align: left\">zarr.storage.DirectoryStore</td></tr><tr><th style=\"text-align: left\">No. members</th><td style=\"text-align: left\">13</td></tr><tr><th style=\"text-align: left\">No. arrays</th><td style=\"text-align: left\">13</td></tr><tr><th style=\"text-align: left\">No. groups</th><td style=\"text-align: left\">0</td></tr><tr><th style=\"text-align: left\">Arrays</th><td style=\"text-align: left\">cloth_aabb, cloth_canonical_aabb, cloth_canonical_verts, cloth_faces, cloth_texture, cloth_uv_faces, cloth_uv_verts, cloth_verts, extrinsic_list, human_canonical_aabb, human_faces, human_verts, intrinsic</td></tr></tbody></table>"
      ],
      "text/plain": [
       "Name        : /Tshirt/samples/00380_Tshirt_509/misc\n",
       "Type        : zarr.hierarchy.Group\n",
       "Read-only   : True\n",
       "Store type  : zarr.storage.DirectoryStore\n",
       "No. members : 13\n",
       "No. arrays  : 13\n",
       "No. groups  : 0\n",
       "Arrays      : cloth_aabb, cloth_canonical_aabb, cloth_canonical_verts,\n",
       "            : cloth_faces, cloth_texture, cloth_uv_faces, cloth_uv_verts,\n",
       "            : cloth_verts, extrinsic_list, human_canonical_aabb, human_faces,\n",
       "            : human_verts, intrinsic"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zr['Tshirt/samples/00380_Tshirt_509/misc'].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
